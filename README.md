# MLSys Literature

---

### Inference
1. Full Stack Optimization of Transformer Inference: a Survey: [https://arxiv.org/pdf/2302.14017.pdf](https://arxiv.org/pdf/2302.14017.pdf)
2. Large Transformer Model Inference Optimization: [https://lilianweng.github.io/posts/2023-01-10-inference-optimization/](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
3. High-throughput Generative Inference of Large Language Models with a Single GPU: [https://arxiv.org/pdf/2303.06865.pdf](https://arxiv.org/pdf/2303.06865.pdf)

### Quantization
1. Up or Down? Adaptive Rounding for Post-Training Quantization: [https://arxiv.org/pdf/2004.10568.pdf](https://arxiv.org/pdf/2004.10568.pdf)
2. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale: [https://arxiv.org/pdf/2208.07339.pdf](https://arxiv.org/pdf/2208.07339.pdf)
3. ULPPACK: FAST SUB-8-BIT MATRIX MULTIPLY ON COMMODITY SIMD HARDWARE: [https://proceedings.mlsys.org/paper/2022/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf](https://proceedings.mlsys.org/paper/2022/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf)

### Training
1. Training Compute-Optimal Large Language Models: [https://arxiv.org/pdf/2203.15556.pdf](https://arxiv.org/pdf/2203.15556.pdf)
2. Decentralized Training of Foundation Models in Heterogeneous Environments: [https://arxiv.org/pdf/2206.01288.pdf](https://arxiv.org/pdf/2206.01288.pdf)

### Scaling
1. Sparse is Enough in Scaling Transformers: [https://proceedings.neurips.cc/paper/2021/file/51f15efdd170e6043fa02a74882f0470-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/51f15efdd170e6043fa02a74882f0470-Paper.pdf)

### Compilation
1. RAF: Holistic Compilation for Deep Learning Model Training: [https://arxiv.org/pdf/2303.04759v1.pdf](https://arxiv.org/pdf/2303.04759v1.pdf)
