# MLSys Literature

---

### Inference
1. Full Stack Optimization of Transformer Inference: a Survey: [https://arxiv.org/pdf/2302.14017.pdf](https://arxiv.org/pdf/2302.14017.pdf)
2. Large Transformer Model Inference Optimization: [https://lilianweng.github.io/posts/2023-01-10-inference-optimization/](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

### Quantization
1. Up or Down? Adaptive Rounding for Post-Training Quantization: [https://arxiv.org/pdf/2004.10568.pdf](https://arxiv.org/pdf/2004.10568.pdf)
2. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale: [https://arxiv.org/pdf/2208.07339.pdf](https://arxiv.org/pdf/2208.07339.pdf)
